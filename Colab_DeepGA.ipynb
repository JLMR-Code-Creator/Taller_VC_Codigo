{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JLMR-Code-Creator/Taller_VC_Codigo/blob/main/Colab_DeepGA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFBKpZCoYD8n"
      },
      "source": [
        "**Installing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqdHL7Z_X84C"
      },
      "source": [
        "!pip install torch torchvision torchaudio torchtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWqw4OXWYCay"
      },
      "source": [
        "!pip install pytorch-forecasting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWXiuMYzYJIk"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laZjSYBuYLER"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch import optim\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import math\n",
        "import scipy.io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjVfGIiqZVKh"
      },
      "source": [
        "**EncodingClass.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eXaGfTqZX0B"
      },
      "source": [
        "'''Hyperparameters configuration'''\n",
        "#Convolutional layers\n",
        "'''Hyperparameters configuration'''\n",
        "#Convolutional layers\n",
        "FSIZES = [2,3,4,5,6]\n",
        "#FSIZES = [2,3,4,5,6,7,8]\n",
        "NFILTERS = [2,4,8,16,32]\n",
        "\n",
        "#Pooling layers\n",
        "PSIZES = [2,3,4,5]\n",
        "PTYPE = ['max', 'avg']\n",
        "\n",
        "#Fully connected layers\n",
        "NEURONS = [4,8,16,32,64,128]\n",
        "\n",
        "class Encoding:\n",
        "    def __init__(self, minC, maxC, minF, maxF):\n",
        "        self.n_conv = random.randint(minC, maxC)\n",
        "        self.n_full = random.randint(minF, maxF)\n",
        "\n",
        "\n",
        "        '''First level encoding'''\n",
        "        self.first_level = []\n",
        "\n",
        "        #Feature extraction part\n",
        "        for i in range(self.n_conv):\n",
        "            layer = {'type' : 'conv',\n",
        "                     'nfilters' : random.choice(NFILTERS),\n",
        "                     'fsize' : random.choice(FSIZES),\n",
        "                     'pool' : random.choice(['max', 'avg', 'off']),\n",
        "                     'psize' : random.choice(PSIZES)\n",
        "                    }\n",
        "            self.first_level.append(layer)\n",
        "\n",
        "        #Fully connected part\n",
        "        for i in range(self.n_full):\n",
        "            layer = {'type' : 'fc',\n",
        "                     'neurons' : random.choice(NEURONS)}\n",
        "\n",
        "            self.first_level.append(layer)\n",
        "\n",
        "\n",
        "        '''Second level encoding'''\n",
        "        self.second_level = []\n",
        "        prev = -1\n",
        "        for i in range(self.n_conv):\n",
        "            if prev < 1:\n",
        "                prev += 1\n",
        "            if prev >= 1:\n",
        "                for _ in range(prev-1):\n",
        "                    self.second_level.append(random.choice([0,1]))\n",
        "                prev += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decondig.py**"
      ],
      "metadata": {
        "id": "IFC3Jg0HL7Hk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnLkivWFZc1j"
      },
      "source": [
        "def conv_out_size(W, K):\n",
        "    return W - K + 3\n",
        "\n",
        "def pool_out_size(W, K):\n",
        "    return math.floor((W - K)/2) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROxmA9hnZekJ"
      },
      "source": [
        "def decoding(encoding):\n",
        "  n_conv = encoding.n_conv\n",
        "  n_full = encoding.n_full\n",
        "  first_level = encoding.first_level\n",
        "  second_level = encoding.second_level\n",
        "\n",
        "  features = []\n",
        "  classifier = []\n",
        "  in_channels = 1\n",
        "  out_size = 256\n",
        "  prev = -1\n",
        "  pos = 0\n",
        "  o_sizes = []\n",
        "  for i in range(n_conv):\n",
        "    layer = first_level[i]\n",
        "    n_filters = layer['nfilters']\n",
        "    f_size = layer['fsize']\n",
        "    pad = 1\n",
        "    if f_size > out_size:\n",
        "        f_size = out_size - 1\n",
        "    if i == 0 or i == 1:\n",
        "      if layer['pool'] == 'off':\n",
        "        operation = [nn.Conv2d(in_channels = in_channels, out_channels = n_filters, kernel_size = f_size, padding = pad),\n",
        "                    nn.BatchNorm2d(n_filters),\n",
        "                    nn.ReLU(inplace = True)]\n",
        "        in_channels = n_filters\n",
        "        out_size = conv_out_size(out_size, f_size)\n",
        "        o_sizes.append([out_size, in_channels])\n",
        "\n",
        "      if layer['pool'] == 'avg':\n",
        "          p_size = layer['psize']\n",
        "          if p_size > out_size:\n",
        "              p_size = out_size - 1\n",
        "          operation = [nn.Conv2d(in_channels = in_channels, out_channels = n_filters, kernel_size = f_size, padding = pad),\n",
        "                      nn.BatchNorm2d(n_filters),\n",
        "                      nn.ReLU(inplace = True),\n",
        "                      nn.AvgPool2d(kernel_size = p_size, stride = 2)]\n",
        "          in_channels = n_filters\n",
        "          out_size = conv_out_size(out_size, f_size)\n",
        "          out_size = pool_out_size(out_size, p_size)\n",
        "          o_sizes.append([out_size, in_channels])\n",
        "\n",
        "      if layer['pool'] == 'max':\n",
        "          p_size = layer['psize']\n",
        "          if p_size > out_size:\n",
        "              p_size = out_size - 1\n",
        "          operation = [nn.Conv2d(in_channels = in_channels, out_channels = n_filters, kernel_size = f_size, padding = pad),\n",
        "                      nn.BatchNorm2d(n_filters),\n",
        "                      nn.ReLU(inplace = True),\n",
        "                      nn.MaxPool2d(kernel_size = p_size, stride = 2)]\n",
        "          in_channels = n_filters\n",
        "          out_size = conv_out_size(out_size, f_size)\n",
        "          out_size = pool_out_size(out_size, p_size)\n",
        "          o_sizes.append([out_size, in_channels])\n",
        "    else:\n",
        "      connections = second_level[pos:pos+prev]\n",
        "      for c in range(len(connections)):\n",
        "        if connections[c] == 1:\n",
        "          in_channels += o_sizes[c][1]\n",
        "\n",
        "      if layer['pool'] == 'off':\n",
        "        operation = [nn.Conv2d(in_channels = in_channels, out_channels = n_filters, kernel_size = f_size, padding = pad),\n",
        "                    nn.BatchNorm2d(n_filters),\n",
        "                    nn.ReLU(inplace = True)]\n",
        "        in_channels = n_filters\n",
        "        out_size = conv_out_size(out_size, f_size)\n",
        "        o_sizes.append([out_size, in_channels])\n",
        "\n",
        "      if layer['pool'] == 'avg':\n",
        "          p_size = layer['psize']\n",
        "          if p_size > out_size:\n",
        "              p_size = out_size - 1\n",
        "          operation = [nn.Conv2d(in_channels = in_channels, out_channels = n_filters, kernel_size = f_size, padding = pad),\n",
        "                      nn.BatchNorm2d(n_filters),\n",
        "                      nn.ReLU(inplace = True),\n",
        "                      nn.AvgPool2d(kernel_size = p_size, stride = 2)]\n",
        "          in_channels = n_filters\n",
        "          out_size = conv_out_size(out_size, f_size)\n",
        "          out_size = pool_out_size(out_size, p_size)\n",
        "          o_sizes.append([out_size, in_channels])\n",
        "\n",
        "      if layer['pool'] == 'max':\n",
        "          p_size = layer['psize']\n",
        "          if p_size > out_size:\n",
        "              p_size = out_size - 1\n",
        "          operation = [nn.Conv2d(in_channels = in_channels, out_channels = n_filters, kernel_size = f_size, padding = pad),\n",
        "                      nn.BatchNorm2d(n_filters),\n",
        "                      nn.ReLU(inplace = True),\n",
        "                      nn.MaxPool2d(kernel_size = p_size, stride = 2)]\n",
        "          in_channels = n_filters\n",
        "          out_size = conv_out_size(out_size, f_size)\n",
        "          out_size = pool_out_size(out_size, p_size)\n",
        "          o_sizes.append([out_size, in_channels])\n",
        "\n",
        "      pos += prev\n",
        "    prev += 1\n",
        "\n",
        "    features.append(operation)\n",
        "  in_size = out_size*out_size*in_channels\n",
        "  for i in range(n_conv,(n_conv + n_full)):\n",
        "    layer = first_level[i]\n",
        "    n_neurons = layer['neurons']\n",
        "    classifier += [nn.Linear(in_size, n_neurons)]\n",
        "    classifier += [nn.ReLU(inplace = True)]\n",
        "    in_size = n_neurons\n",
        "\n",
        "  ##Last layer generates the last neurons for softmax (change this for binary classification)\n",
        "  classifier += [nn.Linear(n_neurons, 3)]\n",
        "\n",
        "  return features, classifier, o_sizes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz3PivL1ZjNq"
      },
      "source": [
        "**Operators.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KRbWus4ZldZ"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def crossover(x, y):\n",
        "    x = deepcopy(x)\n",
        "    y = deepcopy(y)\n",
        "\n",
        "    '''First parent'''\n",
        "    x_nconv = x.n_conv\n",
        "    x_nfull = x.n_full\n",
        "    xblocks = x.first_level\n",
        "    xbinary = x.second_level\n",
        "\n",
        "    '''Second parent'''\n",
        "    y_nconv = y.n_conv\n",
        "    y_nfull = y.n_full\n",
        "    yblocks = y.first_level\n",
        "    ybinary = y.second_level\n",
        "\n",
        "    '''Convolutional part crossover'''\n",
        "    if x_nconv > y_nconv:\n",
        "        k = math.floor(y_nconv/2)\n",
        "        index = list(range(x_nconv))\n",
        "\n",
        "        '''Exchanging the last k blocks of the smaller parent'''\n",
        "        for i in range(k, y_nconv):\n",
        "            block = yblocks[i] #ith block\n",
        "            ix = random.choice(index) #Selecting random index from larger parent\n",
        "            index.remove(ix)\n",
        "\n",
        "            #Exchange of blocks\n",
        "            yblocks[i] = xblocks[ix]\n",
        "            xblocks[ix] = block\n",
        "\n",
        "    if y_nconv > x_nconv:\n",
        "        k = math.floor(x_nconv/2)\n",
        "        index = list(range(y_nconv))\n",
        "\n",
        "        '''Exchanging the last k blocks of the smaller parent'''\n",
        "        for i in range(k, x_nconv):\n",
        "            block = xblocks[i] #ith block\n",
        "            ix = random.choice(index) #Selecting random index from larger parent\n",
        "            index.remove(ix)\n",
        "\n",
        "            #Exchange of blocks\n",
        "            xblocks[i] = yblocks[ix]\n",
        "            yblocks[ix] = block\n",
        "\n",
        "    if x_nconv == y_nconv:\n",
        "        k = math.floor(x_nconv/2)\n",
        "        index = list(range(x_nconv))\n",
        "\n",
        "        x_part = xblocks[k:x_nconv]\n",
        "\n",
        "        '''Exchaning last half of the blocks'''\n",
        "        xblocks[k:x_nconv] = yblocks[k:y_nconv]\n",
        "        yblocks[k:y_nconv] = x_part\n",
        "\n",
        "    '''Fully-connected part'''\n",
        "    if x_nfull > y_nfull:\n",
        "        k = math.floor(y_nfull/2)\n",
        "        index = list(range(x_nconv, x_nconv + x_nfull))\n",
        "\n",
        "        '''Exchanging the last k blocks of the smaller parent'''\n",
        "        for i in range(y_nconv + k, y_nconv + y_nfull):\n",
        "            block = yblocks[i] #ith block\n",
        "            ix = random.choice(index) #Selecting random index from larger parent\n",
        "            index.remove(ix)\n",
        "\n",
        "            #Exchange of blocks\n",
        "            yblocks[i] = xblocks[ix]\n",
        "            xblocks[ix] = block\n",
        "\n",
        "    if y_nfull > x_nfull:\n",
        "        k = math.floor(x_nfull/2)\n",
        "        index = list(range(y_nconv, y_nconv + y_nfull))\n",
        "\n",
        "        '''Exchanging the last k blocks of the smaller parent'''\n",
        "        for i in range(x_nconv + k, x_nconv + x_nfull):\n",
        "            block = xblocks[i] #ith block\n",
        "            ix = random.choice(index) #Selecting random index from larger parent\n",
        "            index.remove(ix)\n",
        "\n",
        "            #Exchange of blocks\n",
        "            xblocks[i] = yblocks[ix]\n",
        "            yblocks[ix] = block\n",
        "\n",
        "    if x_nfull == y_nfull:\n",
        "        k = math.floor(x_nfull/2)\n",
        "\n",
        "        x_part = xblocks[x_nconv + k:x_nconv + x_nfull]\n",
        "        '''Exchaning last half of the blocks'''\n",
        "        xblocks[x_nconv + k:x_nconv + x_nfull] = yblocks[y_nconv + k:y_nconv + y_nfull]\n",
        "        yblocks[y_nconv + k:y_nconv + y_nfull] = x_part\n",
        "\n",
        "    '''Second level'''\n",
        "    if len(xbinary) > len(ybinary):\n",
        "        if len(ybinary) > 1 :\n",
        "            k = random.choice(list(range(1, len(ybinary))))\n",
        "            partition = ybinary[k:]\n",
        "            nbits = len(partition)\n",
        "\n",
        "            if random.uniform(0,1) >= 0.5:\n",
        "                ybinary[k:] = xbinary[len(xbinary) - nbits:len(xbinary)]\n",
        "                xbinary[len(xbinary) - nbits:len(xbinary)] = partition\n",
        "            else:\n",
        "                ybinary[k:] = xbinary[:nbits]\n",
        "                xbinary[:nbits] = partition\n",
        "\n",
        "    if len(ybinary) > len(xbinary):\n",
        "        if len(xbinary) > 1 :\n",
        "            k = random.choice(list(range(len(xbinary))))\n",
        "            partition = xbinary[k:]\n",
        "            nbits = len(partition)\n",
        "\n",
        "            if random.uniform(0,1) >= 0.5:\n",
        "                xbinary[k:] = ybinary[len(ybinary) - nbits:len(ybinary)]\n",
        "                ybinary[len(ybinary) - nbits:len(ybinary)] = partition\n",
        "            else:\n",
        "                xbinary[k:] = ybinary[:nbits]\n",
        "                ybinary[:nbits] = partition\n",
        "\n",
        "    if len(xbinary) == len(ybinary):\n",
        "        if len(xbinary) > 1 :\n",
        "            k = random.choice(list(range(len(xbinary))))\n",
        "            partition = xbinary[k:]\n",
        "\n",
        "            xbinary[k:] = ybinary[k:]\n",
        "            ybinary[k:] = partition\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def mutation(x):\n",
        "    if random.uniform(0,1) < 0.5:\n",
        "        '''Adding a new block'''\n",
        "        if random.uniform(0,1) > 0.5:\n",
        "            #Adding a fully-connected block\n",
        "            layer = {'type' : 'fc',\n",
        "                     'neurons' : random.choice(NEURONS)}\n",
        "\n",
        "            #Choosing a random index to insert the new block\n",
        "            index = list(range(x.n_conv, x.n_conv + x.n_full))\n",
        "            ix = random.choice(index)\n",
        "\n",
        "            x.first_level.insert(ix, layer)\n",
        "            x.n_full += 1\n",
        "\n",
        "        else:\n",
        "            #Adding a convolutional block\n",
        "            layer = {'type' : 'conv',\n",
        "                     'nfilters' : random.choice(NFILTERS),\n",
        "                     'fsize' : random.choice(FSIZES),\n",
        "                     'pool' : random.choice(['max', 'avg', 'off']),\n",
        "                     'psize' : random.choice(PSIZES)\n",
        "                    }\n",
        "            #Choosing a random index to insert the new block\n",
        "            index = list(range(x.n_conv))\n",
        "            ix = random.choice(index)\n",
        "\n",
        "            x.first_level.insert(ix, layer)\n",
        "            x.n_conv += 1\n",
        "\n",
        "            if ix > 1:\n",
        "                new_bits = []\n",
        "                for i in range(ix - 1):\n",
        "                    new_bits.append(random.choice([0,1]))\n",
        "                pos = int(0.5*(ix**2) - 1.5*(ix) + 1)\n",
        "                start = pos + len(new_bits)\n",
        "                for bit in new_bits:\n",
        "                    x.second_level.insert(pos, bit)\n",
        "                    pos += 1\n",
        "\n",
        "                rest = x.n_conv - ix - 1\n",
        "                add = ix\n",
        "                for j in range(rest):\n",
        "                    x.second_level.insert(start+add-1, random.choice([0,1]))\n",
        "                    start += add\n",
        "                    ix += 1\n",
        "\n",
        "            if ix == 0 or ix == 1:\n",
        "                if x.n_conv - 1 == 2:\n",
        "                    x.second_level.append(random.choice([0,1]))\n",
        "                else:\n",
        "                    add = 0\n",
        "                    for i in range(2, x.n_conv):\n",
        "                        pos = int(0.5*(ix**2) - 1.5*(ix) + 1) + add\n",
        "                        x.second_level.insert(pos, random.choice([0,1]))\n",
        "                        add += 1\n",
        "\n",
        "    else:\n",
        "        '''Changing hyperparameters in one block'''\n",
        "        if random.uniform(0,1) > 0.5:\n",
        "            '''Re-starting a fully-connected block'''\n",
        "            index = list(range(x.n_conv, x.n_conv + x.n_full))\n",
        "            ix = random.choice(index)\n",
        "            new_layer = {'type' : 'fc',\n",
        "                         'neurons' : random.choice(NEURONS)}\n",
        "            #Switching fully-connected block\n",
        "            x.first_level[ix] = new_layer\n",
        "\n",
        "        else:\n",
        "            '''Re-starting a convolutional block'''\n",
        "            index = list(range(x.n_conv))\n",
        "            ix = random.choice(index)\n",
        "            new_layer = {'type' : 'conv',\n",
        "                     'nfilters' : random.choice(NFILTERS),\n",
        "                     'fsize' : random.choice(FSIZES),\n",
        "                     'pool' : random.choice(['max', 'avg', 'off']),\n",
        "                     'psize' : random.choice(PSIZES)\n",
        "                    }\n",
        "\n",
        "            #Switching convolutional block\n",
        "            x.first_level[ix] = new_layer\n",
        "\n",
        "        '''Modifying connections in second level'''\n",
        "        if len(x.second_level) > 0:\n",
        "            k = random.choice(list(range(len(x.second_level))))\n",
        "            #Flipping one bit in the second level\n",
        "            if x.second_level[k] == 1:\n",
        "                x.second_level[k] = 0\n",
        "            else:\n",
        "                x.second_level[k] = 1\n",
        "\n",
        "\n",
        "def selection(tournament, style):\n",
        "    '''Stochastic tournament selection'''\n",
        "    if style == 'max':\n",
        "        if random.uniform(0,1) <= 0.8:\n",
        "            p = max(tournament, key = lambda x: x[1])\n",
        "        else:\n",
        "            p = random.choice(tournament)\n",
        "    else:\n",
        "        if random.uniform(0,1) <= 0.8:\n",
        "            p = min(tournament, key = lambda x: x[1])\n",
        "        else:\n",
        "            p = random.choice(tournament)\n",
        "\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrs1qc3JZhcF"
      },
      "source": [
        "'''Networks class'''\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, encoding, features, classifier, sizes, init_weights = True):\n",
        "    super(CNN, self).__init__()\n",
        "    extraction = []\n",
        "    for layer in features:\n",
        "      extraction += layer\n",
        "    self.extraction = nn.Sequential(*extraction)\n",
        "    self.classifier = nn.Sequential(*classifier)\n",
        "    self.features = features\n",
        "    self.second_level = encoding.second_level\n",
        "    self.sizes = sizes\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Feature extraction'''\n",
        "    prev = -1\n",
        "    pos = 0\n",
        "    outputs = {}\n",
        "    features = self.features\n",
        "    #print(x.shape)\n",
        "    for i in range(len(features)):\n",
        "      #print('Layer: ', i)\n",
        "      if i == 0 or i == 1:\n",
        "        x = nn.Sequential(*features[i])(x)\n",
        "        outputs[i] = x\n",
        "        #print(x.shape)\n",
        "\n",
        "      else:\n",
        "        connections = self.second_level[pos:pos+prev]\n",
        "        for c in range(len(connections)):\n",
        "          if connections[c] == 1:\n",
        "            skip_size = self.sizes[c][0] #Size comming from previous layer\n",
        "            req_size = x.shape[2] #Current feature map size\n",
        "            #print('X: ',x.shape)\n",
        "            if skip_size > req_size:\n",
        "              psize = skip_size - req_size + 1\n",
        "              pool = nn.MaxPool2d(kernel_size = psize, stride = 1) #Applying pooling to adjust sizes\n",
        "              x2 = pool(outputs[c])\n",
        "            if skip_size == req_size:\n",
        "              x2 = outputs[c]\n",
        "            if req_size == skip_size + 1:\n",
        "              pool = nn.MaxPool2d(kernel_size = 2, stride = 1, padding = (1,1))\n",
        "              x2 = pool(outputs[c])\n",
        "            if req_size == skip_size + 2:\n",
        "              pad = int((req_size - skip_size)/2)\n",
        "              padding = nn.ZeroPad2d(pad)\n",
        "              x2 = padding(outputs[c])\n",
        "            #print('X2: ',x2.shape)\n",
        "            x = torch.cat((x, x2), axis = 1)\n",
        "\n",
        "        x = nn.Sequential(*features[i])(x)\n",
        "        #print('Out size: ', x.shape)\n",
        "        outputs[i] = x\n",
        "        pos += prev\n",
        "\n",
        "      prev += 1\n",
        "\n",
        "    #print('Classification size: ', x.shape)\n",
        "    x = torch.flatten(x,1)\n",
        "    '''Classification'''\n",
        "    '''for l in self.classifier:\n",
        "      x = l(x)'''\n",
        "    x = self.classifier(x)\n",
        "    #print(x.shape)\n",
        "    return nn.functional.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTbwT-I7ZG6H"
      },
      "source": [
        "**Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QBRp3dIZJcg",
        "outputId": "13700f8a-10ed-4487-b907-ef84486b0a72"
      },
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHqyBSrqZKB2"
      },
      "source": [
        "c_labels = np.ones(50, dtype = np.int8)  # Benigno\n",
        "n_labels = np.zeros(50, dtype = np.int8) # Salud\n",
        "p_labels = np.full((50,), 2) # Cama\n",
        "# Images path\n",
        "c_root = '/content/gdrive/My Drive/tiras_grises/Benigno/benignop'\n",
        "n_root = '/content/gdrive/My Drive/tiras_grises/Salud/Saludp'\n",
        "p_root = '/content/gdrive/My Drive/tiras_grises/CaMa/CaMap'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eenxq50YGjUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(imgs, limit):\n",
        "    val = math.ceil(limit/30)\n",
        "    if not isinstance(imgs[0], list):\n",
        "       plt.figure(figsize=(30,30))\n",
        "       for i in range(len(train_ds)):\n",
        "          sample = train_ds[i]\n",
        "          image = sample['image']\n",
        "          image = image.numpy()\n",
        "          image = image.transpose((1,2,0))\n",
        "          plt.subplot(val,30,i+1)\n",
        "          plt.imshow(image[:,:,0], cmap='gray')\n",
        "          plt.axis('off')\n",
        "          plt.title(sample['label'])\n",
        "          if (i >= limit-1): break\n",
        "       plt.show()"
      ],
      "metadata": {
        "id": "dEJKh6O5cG9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4e6T3hPGi01"
      },
      "source": [
        "**DataReader.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7byf3HzsZOjR"
      },
      "source": [
        "import torchvision.transforms as T\n",
        "class CovidDataset(Dataset):\n",
        "  def __init__(self, root, labels, transform = None):\n",
        "    self.root = root #The folder path\n",
        "    self.labels = labels #Labels array\n",
        "    self.transform = transform #Transform composition\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    p_root = self.root[:]\n",
        "    img_name_p = p_root + str(idx+1) + '.png'\n",
        "    #image_p = cv2.imread(img_name_p, 0)\n",
        "    image_p = np.array(Image.open(img_name_p))\n",
        "    [H, W] = image_p.shape\n",
        "    image_p = image_p.reshape((H,W,1))\n",
        "    p_label = self.labels[idx]\n",
        "    sample = {'image': image_p, 'label': p_label}\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return sample\n",
        "\n",
        "#Class to transform image to tensor\n",
        "class ToTensor(object):\n",
        "  def __call__(self, sample):\n",
        "    image, label = sample['image'], sample['label']\n",
        "\n",
        "    #Swap dimmensions because:\n",
        "    #       numpy image: H x W x C\n",
        "    #       torch image: C x H x W\n",
        "    #print(image.shape)\n",
        "    image = image.transpose((2,0,1))\n",
        "    #print(image.shape)\n",
        "    return {'image':torch.from_numpy(image),\n",
        "            'label':label}\n",
        "\n",
        "def loading_data():\n",
        "    #Loading Datasets\n",
        "    covid_ds = CovidDataset(root = c_root, labels = c_labels, transform = transforms.Compose([ToTensor()]))\n",
        "    normal_ds = CovidDataset(root = n_root, labels = n_labels, transform = transforms.Compose([ToTensor()]))\n",
        "    pneumonia_ds = CovidDataset(root = p_root, labels = p_labels, transform = transforms.Compose([ToTensor()]))\n",
        "\n",
        "    #Merging Covid, normal, and pneumonia Datasets\n",
        "    dataset = torch.utils.data.ConcatDataset([covid_ds, normal_ds, pneumonia_ds])\n",
        "    lengths = [int(len(dataset)*0.7), int(len(dataset)*0.3)+1]\n",
        "    train_ds, test_ds = torch.utils.data.random_split(dataset = dataset, lengths = lengths)\n",
        "\n",
        "    #i = 1836\n",
        "    #Testing\n",
        "    #print(\"Length of Training Dataset: {}\".format(len(train_ds)))\n",
        "    #print(\"Length of Test Dataset: {}\".format(len(test_ds)))\n",
        "    #print(\"Shape of images as tensors: {}\".format(dataset[i]['image'].shape))\n",
        "    #print(\"Label of image i: {}\".format(dataset[i]['label']))\n",
        "\n",
        "    #Creating Dataloaders\n",
        "    train_dl = DataLoader(train_ds, batch_size = 24, shuffle = True)\n",
        "    test_dl = DataLoader(test_ds, batch_size = 24, shuffle = True)\n",
        "\n",
        "    return train_dl, test_dl\n",
        "\n",
        "train_dl, test_dl = loading_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def guardaimagenes(dataset, information, pathFile):\n",
        "   for i in range(len(datasetI)):\n",
        "      sample = datasetI[i]\n",
        "      image = sample['image']\n",
        "      image = image.numpy()\n",
        "      image = image.transpose((1,2,0))\n",
        "      name =  pathFile+information+'/'+information+'_'+str(sample['label'])+'_'+str(i)+'.jpg'\n",
        "      cv2.imwrite(name, image)"
      ],
      "metadata": {
        "id": "utUt1Gji15g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLJ79lVaZ3dB"
      },
      "source": [
        "**DistributedTraining.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llQFTX8HZ5Ht"
      },
      "source": [
        "def loss_batch(loss_func, xb, yb, yb_h, opt = None):\n",
        "  #Obtain the loss\n",
        "  loss = loss_func(yb_h, yb)\n",
        "  #Obtain peformance metric\n",
        "  metric_b = metrics_batch(yb, yb_h)\n",
        "  if opt is not None:\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "  return loss.item(), metric_b\n",
        "  #return metric_b\n",
        "\n",
        "#Helper function to compute the accuracy per mini_batch\n",
        "def metrics_batch(target, output):\n",
        "  #Obtain output class\n",
        "  pred = output.argmax(dim=1, keepdim = True)\n",
        "  #Compare output class with target class\n",
        "  corrects = pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  return corrects\n",
        "\n",
        "#Helper function to compute the loss and metric values for a dataset\n",
        "def loss_epoch(device, model, loss_func, dataset_dl, opt = None):\n",
        "  loss = 0.0\n",
        "  metric = 0.0\n",
        "  len_data = len(dataset_dl.dataset)\n",
        "  for i, data in enumerate(dataset_dl, 0):\n",
        "    #print('batch: ', i)\n",
        "    xb, yb = data['image'], data['label']\n",
        "    xb = xb.type(torch.double).to(device, dtype = torch.float32)\n",
        "    yb = yb.to(device, dtype = torch.long)\n",
        "\n",
        "    #Obtain model output\n",
        "    yb_h = model(xb)\n",
        "\n",
        "    loss_b, metric_b = loss_batch(loss_func, xb, yb, yb_h, opt)\n",
        "    #metric_b = loss_batch(loss_func, xb, yb, yb_h, opt)\n",
        "    loss += loss_b\n",
        "    if metric_b is not None:\n",
        "      metric += metric_b\n",
        "\n",
        "  loss /= len_data\n",
        "  metric /= len_data\n",
        "\n",
        "  return loss, metric\n",
        "  #return metric\n",
        "\n",
        "#Define the training function\n",
        "def train_val(device, epochs, model, opt, loss_func, train_dl, test_dl):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    #print(epoch)\n",
        "    model.train()\n",
        "    train_loss, train_metric = loss_epoch(device, model, loss_func, train_dl, opt)\n",
        "    #train_metric = loss_epoch(model, loss_func, train_dl, opt)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      val_loss, val_metric = loss_epoch(device, model, loss_func, test_dl)\n",
        "      #val_metric = loss_epoch(model, loss_func, test_dl)\n",
        "    accuracy = val_metric\n",
        "\n",
        "    #print(\"Epoch: %d, train loss: %.6f, val loss: %.6f, test accuracy: %.2f\" %(epoch, train_loss, val_loss, accuracy))\n",
        "\n",
        "  return accuracy, model\n",
        "\n",
        "def training(num, device, model, n_epochs, loss_func, train_dl, test_dl, lr, w, max_params):\n",
        "    #Number of parameters\n",
        "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    #Optimizer\n",
        "    opt = optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "    #Obtaining training accuracy\n",
        "    accuracy, _ = train_val(device, n_epochs, model, opt, loss_func, train_dl, test_dl)\n",
        "\n",
        "    #Fitness function based on accuracy and No. of parameters\n",
        "    #f = abs(accuracy - w*(1 - abs((max_params - params)/max_params)))\n",
        "    f = (1 - w)*accuracy + w*((max_params - params)/max_params)\n",
        "    '''if params < max_params:\n",
        "        f = (1 - w)*accuracy + abs(w*((max_params - params)/max_params))'''\n",
        "    '''else:\n",
        "        #f = (1 - w)*accuracy - abs(w*((max_params - params)/max_params))\n",
        "        f = accuracy - abs((max_params - params)/max_params)'''\n",
        "\n",
        "    #Append results to multiprocessing list\n",
        "    #acc_list.append([num, f, accuracy, params])\n",
        "    return num, f, accuracy, params\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieZzagSsYOaj"
      },
      "source": [
        "**Loading GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEdRSBJoYQbx",
        "outputId": "803ac0b2-9b3d-478f-e325-78d967c2fcbd"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device1 = torch.device(\"cuda:0\")\n",
        "print(device1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dXCfouwaYbN"
      },
      "source": [
        "**DeepGA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCsj6taeabkX"
      },
      "source": [
        "#Maximun and minimum numbers of layers to initialize networks\n",
        "min_conv = 2\n",
        "max_conv = 4\n",
        "min_full = 1\n",
        "max_full = 2\n",
        "\n",
        "'''Genetic Algorithm Parameters'''\n",
        "cr = 0.7 #Crossover rate\n",
        "mr = 0.5 #Mutation rate\n",
        "N = 20 #Population size\n",
        "T = 50 #Number of generations\n",
        "t_size = 5 #tournament size\n",
        "w = 0.3 #penalization weight\n",
        "max_params = 3e6\n",
        "num_epochs =  10\n",
        "lr = 1e-3\n",
        "loss_func = nn.NLLLoss(reduction = \"sum\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPU_DA7sbHbe"
      },
      "source": [
        "import timeit\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from multiprocessing import Process, Manager\n",
        "'''Initialize population'''\n",
        "print('Initialize population')\n",
        "\n",
        "#train_dl, test_dl = loading_data()\n",
        "\n",
        "start = timeit.default_timer()\n",
        "pop = []\n",
        "bestAcc = []\n",
        "bestF = []\n",
        "bestParams = []\n",
        "#manager = Manager()\n",
        "while len(pop) < N:\n",
        "    #acc_list = manager.list()\n",
        "\n",
        "    #Creating genomes (genetic encoding)\n",
        "    e1 = Encoding(min_conv,max_conv,min_full,max_full)\n",
        "    #e2 = Encoding(min_conv,max_conv,min_full,max_full)\n",
        "\n",
        "    #Decoding the networks\n",
        "    network1 = decoding(e1)\n",
        "    #network2 = decoding(e2)\n",
        "\n",
        "    #Creating the CNNs\n",
        "    cnn1 = CNN(e1, network1[0], network1[1], network1[2])\n",
        "    #cnn2 = CNN(e2, network2[0], network2[1], network2[2])\n",
        "\n",
        "    #Evaluate individuals\n",
        "    num1, f1, accuracy1, params1 = training ('1', device1, cnn1, num_epochs, loss_func,\n",
        "                                                  train_dl, test_dl, lr, w, max_params)\n",
        "\n",
        "    #training2 = Process(target = training, args = ('2', device2, cnn2, num_epochs, loss_func,\n",
        "    #                                              train_dl, test_dl, lr, w, max_params, acc_list))\n",
        "\n",
        "    #training1.start()\n",
        "    #training2.start()\n",
        "    #training1.join()\n",
        "    #training2.join()\n",
        "\n",
        "    #if acc_list[0][0] == '1':\n",
        "    pop.append([e1, f1, accuracy1, params1])\n",
        "        #pop.append([e2, acc_list[1][1], acc_list[1][2], acc_list[1][3]])\n",
        "    #else:\n",
        "        #pop.append([e2, acc_list[0][1], acc_list[0][2], acc_list[0][3]])\n",
        "        #pop.append([e1, acc_list[1][1], acc_list[1][2], acc_list[1][3]])\n",
        "\n",
        "'''Genetic Algorithm'''\n",
        "for t in range(T):\n",
        "    print('Generation: ', t)\n",
        "\n",
        "    #Parents Selection\n",
        "    parents = []\n",
        "    while len(parents) < int(N/2):\n",
        "        #Tournament Selection\n",
        "        tournament = random.sample(pop, t_size)\n",
        "        p1 = selection(tournament, 'max')\n",
        "        tournament = random.sample(pop, t_size)\n",
        "        p2 = selection(tournament, 'max')\n",
        "        while p1 == p2:\n",
        "            tournament = random.sample(pop, t_size)\n",
        "            p2 = selection(tournament, 'max')\n",
        "\n",
        "        parents.append(p1)\n",
        "        parents.append(p2)\n",
        "\n",
        "    #Reproduction\n",
        "    offspring = []\n",
        "    while len(offspring) < int(N/2):\n",
        "        par = random.sample(parents, 2)\n",
        "        #Crossover + Mutation\n",
        "        if cr >= random.uniform(0,1): #Crossover\n",
        "            p1 = par[0][0]\n",
        "            p2 = par[1][0]\n",
        "            c1, c2 = crossover(p1, p2)\n",
        "\n",
        "            #Mutation\n",
        "            if mr >= random.uniform(0,1):\n",
        "                mutation(c1)\n",
        "\n",
        "            if mr >= random.uniform(0,1):\n",
        "                mutation(c2)\n",
        "\n",
        "            #Evaluate offspring\n",
        "            #acc_list = manager.list()\n",
        "\n",
        "            #Decoding the network\n",
        "            network1 = decoding(c1)\n",
        "            network2 = decoding(c2)\n",
        "\n",
        "            #Creating the CNN\n",
        "            cnn1 = CNN(c1, network1[0], network1[1], network1[2])\n",
        "            cnn2 = CNN(c2, network2[0], network2[1], network2[2])\n",
        "\n",
        "            #Evaluate individuals\n",
        "            num_cnn1, f_cnn1, accuracy_cnn1, params_cnn1 = training('1', device1, cnn1, num_epochs, loss_func,\n",
        "                                                  train_dl, test_dl, lr, w, max_params)\n",
        "            offspring.append([c1, f_cnn1, accuracy_cnn1, params_cnn1])\n",
        "\n",
        "            num_cnn2, f_cnn2, accuracy_cnn2, params_cnn2 = training('2', device1, cnn2, num_epochs, loss_func,\n",
        "                                                  train_dl, test_dl, lr, w, max_params)\n",
        "            offspring.append([c2, f_cnn2, accuracy_cnn2, params_cnn2])\n",
        "            #training1.start()\n",
        "            #training2.start()\n",
        "            #training1.join()\n",
        "            #training2.join()\n",
        "\n",
        "            #if acc_list[0][0] == '1':\n",
        "            #    offspring.append([c1, acc_list[0][1], acc_list[0][2], acc_list[0][3]])\n",
        "            #    offspring.append([c2, acc_list[1][1], acc_list[1][2], acc_list[1][3]])\n",
        "            #else:\n",
        "            #    offspring.append([c2, acc_list[0][1], acc_list[0][2], acc_list[0][3]])\n",
        "            #    offspring.append([c1, acc_list[1][1], acc_list[1][2], acc_list[1][3]])\n",
        "\n",
        "    #Replacement with elitism\n",
        "    pop = pop + offspring\n",
        "    pop.sort(reverse = True, key = lambda x: x[1])\n",
        "    pop = pop[:N]\n",
        "\n",
        "    leader = max(pop, key = lambda x: x[1])\n",
        "    bestAcc.append(leader[2])\n",
        "    bestF.append(leader[1])\n",
        "    bestParams.append(leader[3])\n",
        "\n",
        "\n",
        "    print('Best fitness: ', leader[1])\n",
        "    print('Best accuracy: ', leader[2])\n",
        "    print('Best No. of Params: ', leader[3])\n",
        "    print('No. of Conv. Layers: ', leader[0].n_conv)\n",
        "    print('No. of FC Layers: ', leader[0].n_full)\n",
        "    print('--------------------------------------------')\n",
        "\n",
        "results = pd.DataFrame(list(zip(bestAcc, bestF, bestParams)), columns = ['Accuracy', 'Fitness', 'No. Params'])\n",
        "final_networks = []\n",
        "final_connections = []\n",
        "objects = []\n",
        "for member in pop:\n",
        "    p = member[0]\n",
        "    objects.append(p)\n",
        "    n_conv = p.n_conv\n",
        "    n_full = p.n_full\n",
        "    description = 'The network has ' + str(n_conv) + ' convolutional layers ' + 'with: '\n",
        "    for i in range(n_conv):\n",
        "        nfilters = str(p.first_level[i]['nfilters'])\n",
        "        fsize = str(p.first_level[i]['fsize'])\n",
        "        pool = str(p.first_level[i]['pool'])\n",
        "        psize = str(p.first_level[i]['psize'])\n",
        "        layer = '(' + nfilters + ', ' + fsize + ', ' + pool + ', ' + psize + ') '\n",
        "        description += layer\n",
        "    description += 'and '\n",
        "    description += str(n_full)\n",
        "    description += ' '\n",
        "    description += 'fully-connected layers with: '\n",
        "    for i in range(n_conv, n_conv+n_full):\n",
        "        neurons = str(p.first_level[i]['neurons'])\n",
        "        layer = '(' + neurons + ')'\n",
        "        description += layer\n",
        "    description += ' neurons'\n",
        "    final_networks.append(description)\n",
        "\n",
        "    connections = ''\n",
        "    for bit in p.second_level:\n",
        "        if bit == 1:\n",
        "            connections += 'one - '\n",
        "        if bit == 0:\n",
        "            connections += 'zero - '\n",
        "    final_connections.append(connections)\n",
        "\n",
        "\n",
        "final_population = pd.DataFrame(list(zip(final_networks, final_connections)), columns = ['Network Architecture', 'Connections'])\n",
        "\n",
        "'''Saving Results as CSV'''\n",
        "final_population.to_csv('final_population.csv', index = False)\n",
        "results.to_csv('results.csv', index = False)\n",
        "stop = timeit.default_timer()\n",
        "execution_time = (stop-start)/3600\n",
        "print(\"Execution time: \", execution_time)\n",
        "\n",
        "#Saving objects\n",
        "\n",
        "with open('cnns.pkl', 'wb') as output:\n",
        "    pickle.dump(objects, output, pickle.HIGHEST_PROTOCOL)\n",
        "    output.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_networks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s6zz8ujYLKw",
        "outputId": "5b479749-5b61-4bb4-c742-b3c2115e376e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (32, 4, avg, 4) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (32, 4, avg, 4) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (32, 4, avg, 4) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (32, 4, avg, 4) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (32, 4, avg, 4) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 2 fully-connected layers with: (32)(16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (2, 5, off, 5) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (32, 4, avg, 4) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons',\n",
              " 'The network has 3 convolutional layers with: (4, 2, avg, 2) (8, 3, max, 3) (4, 2, avg, 2) and 1 fully-connected layers with: (16) neurons']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}